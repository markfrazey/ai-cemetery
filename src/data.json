[
  {
    "name": "Tay",
    "creator": "Microsoft",
    "start_date": "3/23/2016",
    "end_date": "3/24/2016",
    "description": "Started scraping harmful and negative (namely anti-Semitic, racist, and sexist) content. Trolls fed the bot offensive and harmful content which the bot learned and got out of control. Was reactivated later on, but started spamming the Twitter timeline.",
    "tags": [
      "Racism",
      "Sexism",
      "Anti-semitism"
    ],
    "links": [
      "https://en.wikipedia.org/wiki/Tay_(bot)",
      "https://web.archive.org/web/20171001033850/http://www.access-ai.com/news/4108/really-need-take-accountability-microsoft-ceo-tay-chatbot/",
      "https://web.archive.org/web/20190725004127/https://www.zdnet.com/article/microsoft-and-the-learnings-from-its-failed-tay-artificial-intelligence-bot/"
    ],
    "timeline": [
      {}
    ]
  },
  {
    "name": "Lee Luda",
    "creator": "ScatterLab",
    "start_date": "12/23/2020",
    "end_date": "1/11/2021",
    "description": "Lee Luda was released in South Korea and quickly gained popularity, as well as controversy. The chatbot was trained on 10 billion conversations from conversations between couples on KakaoTalk. Problems arose since the data was obtained without informing users, as well as sharing the data on GitHub without proper anonymization. The chatbot also expressed produced hate speech towards women, sexual minorities, and people with disabilities. The company behind Lee Luda failed to take responsibility for their poor data ethics, and Lee Luda was shut down and investigated. ",
    "tags": [
      "Privacy",
      "Racism",
      "Sexism",
      "Ableism",
      "Homophobia"
    ],
    "links": [
      "https://medium.com/carre4/ai-chatbot-lee-luda-and-data-ethics-1e523290c816",
      "https://thediplomat.com/2021/01/chatbot-gone-awry-starts-conversations-about-ai-ethics-in-south-korea/"
    ]
  },
  {
    "name": "BabyQ",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "Xiaoice",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "Blender Bot 3",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "Galactica",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "Bard",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "Duplex",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "Replika",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "Bing AI",
    "creator": "Microsoft",
    "start_date": "2/7/2023",
    "end_date": "",
    "description": "Microsoft announced a new version of Bing in February 2023, which is an integration with ChatGPT. Users can ask the chatbot informational questions, as well as prompt it for creative responses such as writing songs or stories. The AI quickly became criticized, with users pointing out its multiple flaws. The chatbot is prone to arguing with users, revealing confidential information, providing factually incorrect answers, as well as declaring its love to users. Microsoft then imposed a session limit on the chatbot, making it so users could only ask six questions and have 60 total chats per day. ",
    "tags": [],
    "links": []
  },
  {
    "name": "",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  },
  {
    "name": "",
    "creator": "",
    "start_date": "",
    "end_date": "",
    "description": "",
    "tags": [],
    "links": []
  }
]